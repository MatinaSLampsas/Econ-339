---
title: "ECON 339 - Tuesday Week 3"
date: "April 16 , 2024"
author: "Matina Lampsas"
format: 
   html:
     embed-resources: true
     code-tools: true
     toc: true
     code-fold: show
editor: source
execute: 
  error: true
  echo: true
  message: false
  warning: false
---

# Setup
```{r setup}
library(tidyverse)
library(readxl)
options(scipen = 999)

```

# Read in the data
```{r}
#read in the data for the college 
retail_data <- read_xlsx(here::here("data",
                     "Data for Weeks 2-7.xlsx"), 
                      sheet = "Retail", 
                     range = "A1:D41"
                      )
```

# Create the Dummy Variables: 
```{r}
#make 3 dummy variables: 
retail_data <- retail_data %>% 
  mutate(d1 = if_else(Quarter == 1, 1,0),
         d2 = if_else(Quarter == 2, 1,0),
         d3 = if_else(Quarter == 3, 1,0))

```

# Make the regression Model
```{r}
retail_lm <- lm(Sales~GNP+d1+d2+d3, data = retail_data)
predict(retail_lm, data.frame(GNP = 18000, d1 = 0, d2= 1, d3 = 0))
coefs <- broom::tidy(retail_lm)
summary(retail_lm)
```
## Interpretation of Slope of d4 :
for every 1 unit increase of GNP, the sales will increase by (`r as.numeric(coefs[2,2])`) billion in Q4


# Predict 
## What are the predicted sales in quarter 2 if GNP is $18,000 (in billions)?
```{r}
predict(retail_lm, data.frame(GNP = 18000, d1 = 0, d2= 1, d3 = 0))
```

## Does using a different quarter for reference impact the inference?
Yes, because the reason d1,d2,d3 are negative is because we use d4 as a reference and the sales are higher in d4 than in other quarters. If we change the reference variable than yes it will impact the inference since the slopes of the dummy variables will change. 

# Homework - probelm: 
First subset the House_Price data by the college town of Ames (Iowa State 
University) or the college town of Lincoln (University of Nebraska). Also, only include Single 
Family houses. You should obtain 612 observations between these two towns, of which 209 are 
in Ames and 403 are in Lincoln.
## Read in the housing data
```{r}
house_price_data <- read_xlsx(here::here("data",
                     "Data for Weeks 2-7.xlsx"), 
                      sheet = "House_Price"
                      )
```

## Question 1:
 Find the averages for the sale amount, number of bedrooms, number of bathrooms, square 
footage, and lot size in both campus towns.
```{r}
avg_ames <- house_price_data %>%
  filter(Town == "Ames, IA") %>%
  summarise(across(.cols = Sale_amount:Sqft_lot,
                   .fns = ~mean(.x)))
avg_lincoln <- house_price_data %>%
  filter(Town == "Lincoln, NE") %>%
  summarise(across(.cols = Sale_amount:Sqft_lot,
                   .fns = ~mean(.x)))

avg_ames
avg_lincoln
# 
#Working on another way to do this using map()
# filtered_house_data %>%
#   select(Town,
#          Sale_amount,
#          Beds,
#          Baths,
#          Sqft_home,
#          Sqft_lot) %>%
#   group_by(Town) %>%
#   summarise(map(.x ,
#     .f = ~mean()))


```
## Question 2/3: 
2. Make the two linear models from the slides
3. Which of the above models is better for making predictions?

```{r}
filtered_house_data <- house_price_data %>% 
  # Filter the housing data just so it only includes the cities Ames, IA and Lincoln, NE. 
  filter(Town == "Lincoln, NE" |
        Town == "Ames, IA",
        Type == "Single Family") %>% 
# create the dummy variables: 
  # Dummy 1 - Ames - gives a value of 1 if the city is ames, and a value of 0 if it isn't 
  # Dummy 2 - newer - provides a value of 1 if the house was built in a year after 2000 and a 0 if it is built before 2000
  mutate(ames = if_else(Town == "Ames, IA", 1, 0),
         newer = if_else(Build_year> 2000, 1,0 ))

#Create the linear models from the HW
lm_1 <- lm(Sale_amount~Beds+Baths+Sqft_home+Sqft_lot+ames, data = filtered_house_data)
lm_2 <- lm(Sale_amount~Beds+Baths+Sqft_home+Sqft_lot+ames+newer, data = filtered_house_data)

broom::tidy(summary(lm_1))
broom::tidy(summary(lm_2))
summary(lm_1)
summary(lm_2)
```

Model 2 is better since it has a higher adjusted R^2. Since we have different numbers of dummy variables, we have to use adjusted R^2 in order to compare the models. 

# Predction & Confidence Interval: 
```{r}
college_data <- read_xlsx(here::here("data",
                     "Data for Weeks 2-7.xlsx"), 
                      sheet = "College",
                     range = "A1:K117"
                      )
```

```{r}
college_lm <- lm(Earnings~`Cost*`+`Grad*`+`Debt*`+`City*`, data = college_data)

summary(college_lm)
college_coefs <- broom::tidy(college_lm)
college_coefs
```

```{r}
confint.default(college_lm, level = 0.95)
```

```{r}

# predict_int <- function(lm, data_set ,con_level){
#   residuals <- resid(lm)
#   residual_sd <- sd(residuals)
#   df <- df.residual(lm)
#   n_obs <- nobs(lm)
#   confidence_level <- con_level
#   t_value <- qt((1 + confidence_level) / 2, df)
#   margin_of_error <- t_value * residual_sd * sqrt(1 + (1/n_obs) + ((new_observation - mean(data_set)) ^ 2) / sum((new_observation - mean(data_set)) ^ 2))
#   lower_bound <- predict(lm, interval = "prediction")[1, "fit"] - margin_of_error
#   upper_bound <- predict(lm, interval = "prediction")[1, "fit"] + margin_of_error
# 
# # Make the prediction interval into a vector
#   prediction_interval <- c(lower_bound, upper_bound)
# return(prediction_interval)

# }

```

```{r}
# prediction_interval <- function(lm_model, data, confidence_level = 0.95) {
#   # Calculate residuals
#   residuals <- resid(lm_model)
#   
#   # Calculate the standard deviation of residuals
#   residual_sd <- sd(residuals)
#   
#   # Degrees of freedom
#   df <- df.residual(lm_model)
#   
#   # Number of observations
#   n_obs <- nrow(data)
#   
#   # Calculate the t-value for the desired confidence level
#   t_value <- qt((1 + confidence_level) / 2, df)
#   
#   # Calculate the margin of error
#   margin_of_error <- t_value * residual_sd * sqrt(1 + (1/df))
#   
#   # Calculate lower and upper bounds of the prediction interval
#   lower_bound <- predict(lm_model, interval = "prediction")[, "fit"] - margin_of_error
#   upper_bound <- predict(lm_model, interval = "prediction")[, "fit"] + margin_of_error
#   
#   # Return the prediction interval
#   return(c(mean(lower_bound), mean(upper_bound)))
# }
# 
# prediction_interval(college_lm, college_data, confidence_level = 0.95)

```
```{r}
calculate_prediction_interval <- function(model, data, confidence_level = 0.95) {
  
  #y_hat: 
  y_hat <- coef(model)[1]
  # Calculate residuals
  residuals <- resid(model)
  
  # Calculate the standard deviation of residuals
  residual_sd <- sd(residuals)
  
  # Calculate degrees of freedom
  df <- df.residual(model)
  
  # # Determine the number of predictors (excluding intercept)
  # n_predictors <- length(coef(model)) - 1  
  
  # # Obtain the total number of observations
  # n_obs <- nobs(model)
  
  # residual standard error:
  resid_se <- summary(model)$sigma
  
  # Calculate the t-value corresponding to the desired confidence level and degrees of freedom
  t_value <- qt((confidence_level) , df)
  sqrt_sum_squares <- sqrt(resid_se^2)
  
  # Calculate the margin of error for prediction interval
  margin_of_error <- t_value * sqrt((resid_se^2 + (coef(model)[2])^2))
  
  # Calculate lower and upper bounds of the prediction interval for the data
  lower_bound <- predict(model, interval = "prediction")[, "fit"] - margin_of_error
  upper_bound <- predict(model, interval = "prediction")[, "fit"] + margin_of_error
  
  # Combine the lower and upper bounds into a prediction interval
  prediction_interval <- c(mean(lower_bound), mean(upper_bound))
  
  # Return the prediction interval
  return(prediction_interval)
}

# Usage example
calculate_prediction_interval(college_lm, college_data, confidence_level = 0.975)


```

